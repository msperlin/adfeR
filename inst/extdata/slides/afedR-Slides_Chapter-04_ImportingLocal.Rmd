---
title: "Analyzing Financial and Economic Data with R"
subtitle: "Chapter 04 - Importing Data from Local Files"
author: "Marcelo S. Perlin"
date: "2020-02-15"
output:
  slidy_presentation: default
  ioslides_presentation: default
  beamer_presentation: default

---

## Introduction {#introduction}

```{r, include=FALSE}
my.fig.height <- 3
my.fig.width <- 4
my.out.width <- '100%'
book.site.zip <- 'https://www.msperlin.com/blog/static/afedr-files/afedr_files.zip'
```

In this class, we will learn to import and export data available as local files in the computer. 

Here we will draw a comprehensive list of file formats for importing and exporting data, including:

- Text data with comma-separated values (_csv_);
- Microsoft Excel (_xls_, _xlsx_);
- R native files (_RData_, _rds_);
- `fst` format;
- SQLite;
- Unstructured text data.

The first lesson in importing data from local files is that the location of the file must be explicitly stated in the code. An example:

```{r}
my_file <- 'C:/My Research/data/SP500_Data.csv'
```

Note the use of forwarding slashes (`/`) to designate the file directory. Relative references also work, as in:

```{r}
my_file <- 'data/SP500_Data.csv'
```

Here, it is assumed that in the current working folder there is a directory called `data` and, inside of it, a file called `SP500_Data.csv`. 

**The data will be imported and exported in R as an object of type `dataframe`**. That is, a table contained in an Excel or _.csv_  file will become a `dataframe` object in R. When we export data, the most common format is this same type of object. Conveniently, `dataframes` are nothing more than tables, with rows and columns. 

## _csv_ files 

```{r}
# get sp500 data
my_f <- afedR::afedR_get_data_file('SP500.csv')

first_date <- as.Date('2010-01-01')
last_date <- as.Date('2019-01-01')
```

Consider the data file called `r basename(my_f)`. It contains daily closing prices of the SP500 index from `r first_date` until `r last_date`. 

Here we will use package `afedR` for grabbing the file and copying it to your local folder. If you followed the instructions in the book preface chapter, you should have package `afedR` already installed. If not, execute the following code:

```{r, eval=FALSE}
# install devtools dependency
install.packages('devtools')

# install book package
devtools::install_github('msperlin/afedR')
```

Once you installed package `afedR`, file `r basename(my_f)` and all other data files used in the book were downloaded from Github. Command `afedR::afedR_get_data_file` will return the local path of a book data file by its name. 

Let's copy `r basename(my_f)` to your "My Documents" folder with the following code using the tilde (`~`) shortcut:

```{r, eval=FALSE}
my_f <- afedR::afedR_get_data_file('SP500.csv')
file.copy(from = my_f, to = '~' )
```

The content of `r basename(my_f)` is very standard. However, you should be aware this is not always the case. So, if you want to avoid the common issues, I suggest that you use a set of steps that can avoid most problems in importing data from _.csv_ files: 

1) Check the existence of text before the actual data. A standard _.csv_ file will only have the contents of a table but, sometimes, you will find a header text with some details about the data. In R, you can control how many lines you skip in the _csv_ reading function;

2) Verify the existence of names for all columns and if those names are readable;
   
3) Check the symbol for column separation. Normally it is a comma, but you never know for sure;
   
4) For the numerical data, verify the decimal symbol. R will expect it to be a dot. If necessary, you can adjust this information in the CSV reading function. 

5) Check the encoding of the text file. Normally it is one of UTF-8, Latin1 (ISO-8859) or Windows 1252. These are broad encoding formats and should suffice for most languages. Whenever you find strange symbols in the text columns of the resulting `dataframe`, the problem is due to a difference in encoding. While the file is encoded in a specific format, R is reading it with a different encoding structure. Windows users can check the encoding of a text file by opening it in [Notepad++](https://notepad-plus-plus.org/)^[https://notepad-plus-plus.org/]. The information about the encoding will be available in the bottom right corner of the Notepad++ editor. However, you need to be aware that Notepad++ is not part of the Windows installation and you might need to install it on your computer. Linux and Mac users can find the same information in any advanced text editor software such as [Kate](https://kate-editor.org/)^[https://kate-editor.org/].

Whenever you find an unexpected text structure in a _.csv_ file, use the arguments of the _csv_ reading function to import the information correctly. 


### Importing Data

The `base` package of R includes a native function called `read.csv` for importing data from _.csv_ files. However, we will prefer the `tidyverse` alternative, `readr::read_csv`, as it is more efficient and easier to work with. In short, the benefit is that it reads the data very quickly, and it uses clever rules for defining the classes of imported columns. 

This is the first package from the `tidyverse` that we will use. Before doing so, it is necessary to install it in your R session. A simple way of installing all `tidyverse` packages as a bundle is as follows:

```{r, eval=FALSE}
install.packages('tidyverse')
```

After running the previous code, all `tidyverse` packages will be installed on your computer. You should also keep in mind that aspects such installation might take a while. Once it finishes, let's load the `tidyverse` set of packages.

```{r}
# load library
library(tidyverse)
```

Back to importing data from _.csv_ files, to load the contents of file `r basename(my_f)` in R, use the `readr::read_csv` function. \index{readr!read\_csv}

```{r, message=TRUE}
# set file to read
my_f <- afedR::afedR_get_data_file('SP500.csv')

# read file
my_df_sp500 <- read_csv(my_f)

# print it
print(head(my_df_sp500))
```

The contents of the imported file are set as a `dataframe` object in R. As mentioned in the previous chapter, each column of a `dataframe` has a class. We can check the classes of `my_df_sp500` using function `glimpse` from package `dplyr`, which is also part of the `tidyverse`. This function is an improved version of `base:str`, also showing a textual representation of R objects. Let's use it:

```{r}
# Check the content of dataframe
glimpse(my_df_sp500)
```

Note that the column of dates (`date`) was imported as a `Date` vector and the closing prices as numeric (`dbl`, double accuracy). This is exactly what we expected. Internally, function `read_csv` identifies columns classes according to their content. 

There is also a simpler way of using the classes defined by `read_csv`, just set `col_types = cols()`:

```{r}
# read file with readr::read_csv
my_df_sp500 <- read_csv(my_f, 
                        col_types = cols())

# glimpse the dataframe
glimpse(my_df_sp500)
```


### Exporting Data

To write a _.csv_ file, use the `readr::write_csv` function. First, we create a new dataframe with some random data:  \index{file types!.csv} \index{base!write.csv}

```{r}
# set the number of rows
N <- 100

# set dataframe
my_df <- data.frame(y = runif(N), 
                    z = rep('a',N))

# print it
print(head(my_df))
```

And now we use `write_csv` to save it in a new _.csv_ file:

```{r}
# set file out
f_out <- file.path(tempdir(), 'temp.csv')

# write to files
write_csv(x = my_df,  
          path = f_out)
```

In the previous example, we save the object `my_df` into a file with path `data/temp.csv`.


## _Excel_ Files (_xls_ and _xlsx_) 

Although it is not an efficient or portable data storage format, Excel is a very popular software due to its spreadsheet-like capacities. It is not uncommon for data to be stored and distributed in this format, especially in the finance industry. 

The downside of using Excel files for storing data is its low portability and the longer time required to read and write it.  


### Importing Data

R does not have a native function for importing Excel files; therefore, we must install and use packages to perform this operation. There are several options, but the main packages are `XLConnect` [@xlsconnect2016], `xlsx` [@xlsx2014], `readxl` [@readxl2016] and `tidyxl` [@tidyxl]. \index{XLConnet} \index{xlsx} \index{readxl}

In this section, we will give priority to package `readxl`, one of the most straightforward packages to use. 

We can import the information from the file using function `read_excel` from `readxl`: \index{readxl!read\_excel}

```{r}
library(readxl)

# set excel file
my_f <- afedR::afedR_get_data_file('SP500_Excel.xlsx')

# read excel file 
my_df <- read_excel(my_f, sheet = 'Sheet1')

# print classes
print(sapply(my_df, class))

# print with head (first five rows)
print(head(my_df))
```

As we can see, one benefit of using Excel files is that the column's classes are directly inherited. 


### Exporting Data 

Exporting a `dataframe` to an Excel file is also easy. Again, no native function in R performs this procedure. We can, however, use packages `xlsx` and `writexl`:

```{r}
library(xlsx)

# create dataframe
N <- 50
my_df <- data.frame(y = seq(1,N), z = rep('a',N))

# set excel file
f_out <- file.path(tempdir(), 'temp.xlsx')

# write to excel
write.xlsx(x = my_df, file = f_out, sheetName = "my df")
```

As for package `writexl`, its innovation is that a Java installation is not needed. Writing speed is also significantly increased. See an example next. \index{writexl!write\_xlsx}

```{r, eval=FALSE}
library(writexl)
# set number of rows
N <- 25

# create random dfs
my_df_A <- data.frame(y = seq(1, N),
                      z = rep('a', N))

write_xlsx(x = my_df_A,
           path = f_out)
```

In order to compare writing performance, let's calculate the difference of time from `xlsx` to `writexl`:

```{r}
library(writexl)
library(readxl)
library(xlsx)

# set number of rows
N <- 2500

# create random dfs
my_df_A <- data.frame(y = seq(1,N),
                      z = rep('a',N))

# set files
my_file_1 <- file.path(tempdir(), 'temp_writexl.xlsx')
my_file_2 <- file.path(tempdir(),'temp_xlsx.xlsx')

# test export
time_write_writexl <- system.time(write_xlsx(x = my_df_A,
                                             path = my_file_1))

time_write_xlsx <- system.time(write.xlsx(x = my_df_A,
                                          file = my_file_2))

# test read
time_read_readxl <- system.time(read_xlsx(path = my_file_1 ))
time_read_xlsx <- system.time(read.xlsx(file = my_file_2,
                                        sheetIndex = 1 ))
```

And now we show the results:

```{r, results='hold'}
# results
my_formats <- c('xlsx', 'readxl')
results_read <- c(time_read_xlsx[3], time_read_readxl[3])
results_write<- c(time_write_xlsx[3], time_write_writexl[3])

# print text
my_text <- paste0('\nTime to WRITE dataframe with ',
                  my_formats, ': ',
                  format(results_write, digits = 4),
                  ' seconds', collapse = '')
cat(my_text)

my_text <- paste0('\nTime to READ dataframe with ',
                  my_formats, ': ',
                  format(results_read, digits = 4),
                  ' seconds', collapse = '')
cat(my_text)
```

As we can see, even for low-volume data, a dataframe with `r N` rows and `r ncol(my_df_A)` columns, the run-time difference is significant. If you are working with large spreadsheets, the use of packages `readxl` and `writexl` for reading and writing Excel files is strongly recommended. 


## _RData_ and _rds_ Files

R offers native formats to write objects to a local file. The great benefit of using both native formats, _RData_, and _rds_, is that the saved file is compact and its access is very fast. The downside is the low portability, i.e., it's difficult to use the files in other software.


### Importing Data

To create a new _.RData_ file, use the `save` function:

```{r}
# set a object
my_x <- 1:100

# set name of RData file
my_file <- file.path(tempdir(), 'temp.RData')

# save it
save(list = c('my_x'), file = my_file)
```

We can verify the existence of the file with the `file.exists` function: \index{base!list.files}

```{r}
# check if file exists
file.exists(my_file)
```

As expected, file `r basename(my_file)` is available. 

Importing data from `.rds` files is very similar. For that we use function `readr::read_rds`:

```{r, eval=FALSE}
# set file path
my_file <- file.path(tempdir(), 'temp.rds')

# load content into workspace
my_y <- read_rds(path = my_file)
```

Comparing the code between using `.RData` and `.rds` files note that the `.rds` the format allows the explicit definition of the output object. 


### Exporting Data

We can create a new _RData_ file with command `save`:

```{r}
# set vars
my_x <- 1:100
my_y <- 1:100

# write to RData
my_file <- file.path(tempdir(), 'temp.RData')
save(list = c('my_x', 'my_y'),
     file = my_file)
```

We can check if the file exists with function `file.exists`:

```{r}
file.exists(my_file)
```

The result is `TRUE` as expected. 

As for _.rds_ files, we save it with function `readr::write_rds`:

```{r}
# set data and file
my_x <- 1:100
my_file <- file.path(tempdir(), 'temp.rds')

# save as .rds
write_rds(x = my_x,
          path = my_file)

# read it
my_x2 <- read_rds(path = my_file)

# test equality
print(identical(my_x, my_x2))
```

Command `identical` tests if both objects are equal. Again, as expected, we find the result to be `TRUE`. 


## _fst_ files 

The [_fst_ format](http://www.fstpackage.org/)^[http://www.fstpackage.org/] is specially designed to enable quick writing and reading time from tabular data, with minimal disk space. 


### Importing Data

Using _fst_ file is very simple and similar to the previous cases. We use function `read_fst` from package `fst` to read files:

```{r}
library(fst)

# set file location
my_file <- afedR::afedR_get_data_file('temp.fst')

# read fst file
my_df <- read_fst(my_file)

# check contents
glimpse(my_df)
```

As with the other cases, the data from file `r basename(my_file)` is available in the workspace. \index{fst!read\_fst}


### Exporting Data

We use function `fst::write_fst` to save dataframes in the _fst_ format: \index{fst!write\_fst}

```{r}
library(fst)

# create dataframe
N <- 1000
my_file <- file.path(tempdir(), 'temp.fst')
my_df <- data.frame(x = runif(N))

# write to fst
write_fst(x = my_df, path = my_file)
```


### Timing the _fst_ format

As a test of the potential of the `fst` format, we will now time the read and write time between `fst` and `rds` for a large table: 5,000,000 rows and 2 columns. We will also report the size of the resulting file.

```{r}
library(fst)

# set number of rows
N <- 5000000

# create random dfs
my_df <- data.frame(y = seq(1,N),
                    z = rep('a',N))

# set files
my_file_1 <- file.path(tempdir(), 'temp_rds.rds')
my_file_2 <- file.path(tempdir(), 'temp_fst.fst')

# test write
time_write_rds <- system.time(write_rds(my_df, my_file_1 ))
time_write_fst <- system.time(write_fst(my_df, my_file_2 ))

# test read
time_read_rds <- system.time(readRDS(my_file_1))
time_read_fst <- system.time(read_fst(my_file_2))

# test file size (MB)
file_size_rds <- file.size(my_file_1)/1000000
file_size_fst <- file.size(my_file_2)/1000000
```

And now we check the results:

```{r, results='hold'}
# results
my_formats <- c('.rds', '.fst')
results_read <- c(time_read_rds[3], time_read_fst[3])
results_write<- c(time_write_rds[3], time_write_fst[3])
results_file_size <- c(file_size_rds , file_size_fst)

# print text
my_text <- paste0('\nTime to WRITE dataframe with ',
                  my_formats, ': ',
                  results_write, ' seconds', collapse = '')
cat(my_text)

my_text <- paste0('\nTime to READ dataframe with ',
                  my_formats, ': ',
                  results_read, ' seconds', collapse = '')
cat(my_text)

my_text <- paste0('\nResulting FILE SIZE for ',
                  my_formats, ': ',
                  results_file_size, ' MBs', collapse = '')
cat(my_text)

```

The difference is very impressive! The `fst` not only reads and writes faster but also results in smaller file sizes. 


## SQLite Files

Before moving to the examples, we need to understand how to use database software. 

1) R will connect to the database and return a connection object.

2) Based on this connection, we will send queries for importing data using the _SQL_ language. 

3) The software will communicate with R and import the desired table. 


### Importing Data

Assuming the existence of an SQLite file in the computer, we can import its tables with package `RSQLite`:

```{r}
library(RSQLite)

# set name of SQLITE file
f_sqlite <- afedR::afedR_get_data_file('SQLite_db.SQLITE')

# open connection
my_con <- dbConnect(drv = SQLite(), f_sqlite)

# read table
my_df <- dbReadTable(conn = my_con,
                     name = 'MyTable1') # name of table in sqlite

# print with str
glimpse(my_df)
```

It worked. The `dataframe` from the table `MyTable1` is exactly as expected. \index{RSQLite!dbReadTable}

Another example of using SQLite is with the actual SQL statements. Notice, in the previous code, we used the function `dbReadTable` to get the contents of all rows in table `MyTable1`. Now, let's use an SQL command to get from `MyTable2` only the rows where the `G` column is equal to `A`. 

```{r}
# set sql statement
my_SQL_statement <- "select * from myTable2 where G='A'"

# get query
my_df_A <- dbGetQuery(conn = my_con, 
                      statement = my_SQL_statement)

# disconnect from db
dbDisconnect(my_con)

# print with str
print(str(my_df_A))
```

It also worked, as expected. 


### Exporting Data

As an example of exporting data to an SQLite file, let's first create an SQLite database. For that, we will set two large `dataframes` with random data and save both in an SQLite file using the package `RSQLite`. \index{RSQLite}

```{r, echo=FALSE, message=FALSE, purl=FALSE}
f_sqlite <- file.path(tempdir(), 'SQLite_db.SQLITE')
if (file.exists(f_sqlite)) temp <- file.remove(f_sqlite)
```

```{r, tidy=FALSE}
library(RSQLite)

# set number of rows in df
N = 10^6 

# create simulated dataframe
my_large_df_1 <- data.frame(x=runif(N), 
                            G= sample(c('A','B'),
                                      size = N,
                                      replace = TRUE))

my_large_df_2 <- data.frame(x=runif(N), 
                            G = sample(c('A','B'),
                                       size = N,
                                       replace = TRUE))

# set name of SQLITE file
f_sqlite <- file.path(tempdir(), 'SQLite_db.SQLITE')

# open connection
my_con <- dbConnect(drv = SQLite(), f_sqlite)

# write df to sqlite
dbWriteTable(conn = my_con, name = 'MyTable1', 
             value = my_large_df_1)
dbWriteTable(conn = my_con, name = 'MyTable2', 
             value = my_large_df_2)

# disconnect
dbDisconnect(my_con)
```

The `TRUE` output of `dbWriteTable` indicates everything went well. A connection was opened using function `dbConnect`, and the `dataframes` were written to an SQLite file, called `r f_sqlite`. Unlike other database software, SQLite stores data and configurations from a single file, without the need of a formal server. 


## Unstructured Data and Other Formats

One example is the case of unstructured data stored in text files. Let's explore it.


### Importing Data

You can read the contents of a text file with function `readr::read_lines`: \index{readr!read\_lines}

```{r}
# set file to read
my_f <- afedR::afedR_get_data_file('pride_and_prejudice.txt')

# read file line by line
my_txt <- read_lines(my_f)

# print 50 characters of first fifteen lines
print(str_sub(string = my_txt[1:15], 
              start = 1, 
              end = 50))
```

In this example, file `r basename(my_f)` contains the whole content of the book _Pride and Prejudice_ by Jane Austen, freely available in the [Gutenberg](http://www.gutenberg.org/)^[http://www.gutenberg.org/] project. We imported the entire content of the file as a `character` vector named `my_txt`. Each element of `my_txt` is a line from the raw text file. Based on it, we can calculate many things such as the number of lines in the book and the number of times that the name `'Bennet'`, one of the protagonists, appears in the text:

```{r}
# count number of lines
n_lines <- length(my_txt)

# set target text
name_to_search <- 'Bennet'
  
# set function for counting words
fct_count_bennet <- function(str_in, target_text) {
  
  require(stringr)
  

  n_words <- length(str_locate_all(string = str_in, 
                                   pattern = target_text)[[1]])
  
  return(n_words)
}

# use fct for all lines of Pride and Prejudice
n_times <- sum(sapply(X = my_txt, 
                      FUN = fct_count_bennet, 
                      target_text = name_to_search))

# print results
my_msg <- paste0('The number of lines found in the file is ', 
                 n_lines, '.\n',
                 'The word "', name_to_search, '" appears ', 
                 n_times, ' in the book.')
cat(my_msg)
```

In the example, we once again used `sapply`. In this case, it allowed us to use a function for each element of `my_txt`. We searched and counted the number of times the word  "`r name_to_search`" was found. Notice we could simply change `name_to_search` for any other name if we wanted to. 


### Exporting Data

A typical case of exporting unstructured text is saving the log record of a procedure. This is quite simple. Using function `readr::write_lines`, use the input `path` to set the name of the local file and `x` for the actual textual content.  \index{base!cat}

```{r}
# set file
my_f <- file.path(tempdir(), 'temp.txt')

# set some string
my_text <- paste0('Today is ', Sys.Date(), '\n', 
                  'Tomorrow is ', Sys.Date()+1)

# save string to file
write_lines(x = my_text, path = my_f, append = FALSE)
```

In the previous example, we created a simple text object and saved it in `r my_f`. We can check the result with the `readr::read_lines` function:

```{r}
print(read_lines(my_f))
```

As we can see, it worked as expected.


## Exercises {#exerc-importacao-exportacao}

01. Create a dataframe with the following code:

```{r, eval=FALSE}
my_N <- 10000
my_df <- data.frame(x = 1:my_N,
                    y = runif(my_N))
```

Export the resulting dataframe to each of the following formats: csv, rds, xlsx e fst.

Which format took more computer space? Tip: You can use function `file.size` to check the resulting file sizes. 

02. Improve the previous code by measuring the execution time needed to record all data in the different formats. Which one had the minimum writing time? Tip: Use function `system.time` for measuring execution time. 

03. Within the previous code, change the value of `my_N` to `1000000` and re-execute the code. Does this change modify the answers to the two previous questions? How?

04. Using functions `afedR::afedR_get_data_file` and `readr::read_csv`, read the contents of the book file `'SP500-Stocks_long.csv'`. How many columns and how many rows you find in this table?

05. At link [https://eeecon.uibk.ac.at/~zeileis/grunfeld/Grunfeld.csv](https://eeecon.uibk.ac.at/~zeileis/grunfeld/Grunfeld.csv) you can find the CSV file for the _Grunfeld_ data. This is a particularly famous table due to its use as a benchmark data in econometric models. Using function `readr::read_csv`, read this file using the direct link in `read_csv`. How many rows do you find in the resulting `dataframe`?

06. **CHALLENGE** - In the following link:

[https://perso.telecom-paristech.fr/eagan/class/igr204/data/nat1900-2017.tsv](https://perso.telecom-paristech.fr/eagan/class/igr204/data/nat1900-2017.tsv)

you can find data about all baby names in France from 1900 to 2017. But, the data comes with a caveat: all columns are separated by a tab symbol. After reading the manual for `readr::read_delim`, import the information from this file on your computer. How many rows does the file contain?
