---
title: "Analyzing Financial and Economic Data with R"
subtitle: "Chapter 09 - Cleaning and Structuring Data"
author: "Marcelo S. Perlin"
date: "2020-02-15"
output:
  slidy_presentation: default
  ioslides_presentation: default
  beamer_presentation: default

---

## Introduction {#introduction}

```{r, include=FALSE}
my.fig.height <- 3
my.fig.width <- 4
my.out.width <- '100%'
book.site.zip <- 'https://www.msperlin.com/blog/static/afedr-files/afedr_files.zip'

format.cash <- function(x) {
  require(scales)

  x.formatted <- dollar(x,
                        prefix = '$',
                        decimal.mark = '.',
                        big.mark = ',',
                        largest_with_cents = Inf)

  return(x.formatted)
}

```

In this chapter, we will cover basic financial and economic data cleansing and restructuring operations. This includes:

- Changing the format of a dataframe (long/wide);
- Converting a `list` of `dataframes` into a single table;
- Identifying and treating extreme values (_outliers_);
- Price data deflation;
- Data aggregation based on a change of time-frequency (e.g. from daily to monthly).


## The Format of a `dataframe`

The `dataframe` format discussion arose due to the introduction of `tidyverse`. At the heart of the debate, we discuss whether the data should be guided by columns (_wide_ format) or lines (_long_ format).  \index{long format} \index{wide format}

**In the wide format**, the rows of the table are usually indexed by a single factor, such as a date, and the columns indicate the different variables. An example:

```{r, echo=FALSE}
library(tidyverse)

set.seed(10)
N <- 4

temp_df <- tibble(refdate=Sys.Date()+1:N,
                  STOCK1 = 10+cumsum(rnorm(N, sd = 1.25)),
                  STOCK2 = 3+ cumsum(rnorm(N, sd = 0.5)),
                  STOCK3 = 6+ cumsum(rnorm(N, sd = 0.5)))

knitr::kable(temp_df, digits = 2)
```

**In the long format**, each row of the `dataframe` represents a slice of the database and each column is a variable. When new data arrives, the table usually grows in the number of rows. Example:

```{r, echo=FALSE}
wide_df <- tidyr::gather(data = temp_df,
                         key = 'ticker',
                         value = 'price',
                         - refdate)
colnames(wide_df) <- c('refdate', 'ticker', 'price')

knitr::kable(wide_df, digits = 2)
```

### Converting a `dataframe` Structure (long and wide)

The conversion from one format to the other is possible with the `tidyr` package [@tidyr].

```{r, tidy=FALSE}
library(tidyr)
library(tidyverse)

# set dates and stock vectors
refdate <- as.Date('2015-01-01') + 0:3
STOCK1 <- c(10, 11, 10.5, 12)
STOCK2 <- c(3, 3.1, 3.2, 3.5)
STOCK3 <- c(6, 7, 7.5, 6)

# create wide dataframe
my_df_wide <- tibble(refdate, STOCK1, STOCK2, STOCK3)

# print it
print(my_df_wide)

# convert wide to long
my_df_long <- gather(data = my_df_wide,
                     key = 'ticker',
                     value = 'price',
                     - refdate)

# print result
print(my_df_long)
```

To perform the reverse conversion, _long_ to _wide_, we can use the `spread` function from the same package: \index{tidyr!spread}

```{r, tidy=FALSE}
# convert from long to wide
my_df_wide_converted <- spread(data = my_df_long, 
                               key = 'ticker',
                               value = 'price')

# print result
print(my_df_wide_converted)
```


## Converting `lists` into `dataframes`

Another important case in data structuring is the situation where multiple `dataframes` are allocated into a single or multiple `list` object. Such an occurrence is common in two cases: 

1) when we are importing data from many local files 

2) when using particular packages for importing multiple series from the internet. Here we will look at an example for each.

For the first, let's use the `purrr` package as well as a custom defined function to create some files with random data. Here we will take the data generating process one step further when using `wakefield`, an excellent package for creating artificial data. 

```{r, include=FALSE}
# clean up files
file.remove(list.files('many_datafiles_2/', 
                       full.names = TRUE) )
```

```{r}
create_rnd_data <- function(n_obs = 100,
                            folder_out) {
  # function for creating random datasets
  #
  # ARGS: n_obs - number of observations
  #       folder_out - folder where to save files
  #
  # RETURN: TRUE, if sucessfull
  
  require(tidyverse)
  require(wakefield)
  
  # check if folder exists
  if (!dir.exists(folder_out)) dir.create(folder_out)
  
  # create extensive random data
  rnd_df <- r_data_frame(n = n_obs,
                         id,
                         race,
                         age,
                         sex) %>%
    r_na(prob = 0.1)
  
  # for 15% of the time, create a new column
  if (runif(1) < 0.15 ) {
    rnd_df$bad_column <- 'BAD COLUMN!'
  }
  
  # set file name
  f_out <- tempfile(fileext = '.csv', 
                    pattern = 'file_',
                    tmpdir = folder_out)
  
  write_csv(x = rnd_df, 
            path = f_out)
  
  return(TRUE)
}
```

Going forward, let's use `purrr::pmap` to create several files.

```{r}
n_files <- 50
n_obs <- 100
folder_out <- 'many_datafiles_2'

# create random datasets
l_out <- pmap(.l = list(n_obs = rep(n_obs, n_files),
                        folder_out = rep(folder_out, n_files)), 
              .f = create_rnd_data) 

# check if files are there
print(head(list.files(folder_out)))
```

The files are available, as expected. Now, let's create a function that will read each file and output a dataframe:

```{r}
read_single_file <- function(f_in) {
  # Function for reading single csv file with random data
  #
  # ARGS: f_in - path of file
  #
  # RETURN: A dataframe with the data
  
  require(tidyverse)
  
  df <- read_csv(f_in, col_types = cols())
  
  return(df)
}
```

```{r}
library(purrr)

files_to_read <- list.files('many_datafiles_2/', 
                            full.names = TRUE)

l_out <- map(files_to_read, read_single_file)
```

And now we bind them all together with a simple call to `dplyr::bind_rows`:

```{r}
compiled_df <- bind_rows(l_out)

glimpse(compiled_df)
```

For the second example, let's take a case of data structuring using  `BETS`, a package for downloading economic series. 

```{r, cache=TRUE}
library(BETS)

my_id <- 3785:3791

# set dates
first_date = '2010-01-01'
last_date  = as.character(Sys.Date())

# get data
l_out <- BETSget(code = my_id, data.frame = TRUE,
                 from = first_date, to = last_date)

# check data in first dataframe
glimpse(l_out[[1]])
```

In this example we gather data for unemployment rates for `r english::english(length(my_id))` countries. Each dataset is an element in `l_out`. Looking at the data itself, we have two columns: `date` and `value`. 
Now, if we want to structure all imported tables, we will first create another column, `country`, which will store the information of where the unemployment rate was registered.

```{r}
my_countries <- c("Germany", "Canada", "United States", 
                  "France",  "Italy", "Japan", 
                  "United Kingdom")
```

The order of elements in vector `my_countries` follows the order of countries in `l_out`. This is important as we will use it later for matching the information.

Going further, we now create a function that will organize the tables. It will take two inputs, a `dataframe` and the name of the country:

```{r}
clean_bets <- function(df_in, country_in) {
  # function for cleaning data from BETS
  #
  # ARGS: df_in - dataframe within a list
  #       country_in - name of country
  #
  # VALUE: a new dataframe with new column type
  
  #set column
  df_in$country <- country_in
  
  # return df
  return(df_in)
}
```

The next step is to use the previous function to create another list with the organized dataframes. We need to call `clean_bets` for each element of `l_out`, together with its country name. For that, we again use `purrr::pmap` (see chapter \@ref(programming)).

```{r}
library(purrr)

# set args
l_args <- list(df_in = l_out, 
               country_in = my_countries)
# format dfs
l_out_formatted <- pmap(.l = l_args, 
                        .f = clean_bets)

# check first element (all are the same structure)
glimpse(l_out_formatted[[1]])
```

From the output of `glimpse` we see that the column `country` was added to each dataframe. The next and final step is to bind all elements of `l_out_formatted` into a single dataframe with `dplyr::bind_rows`:

```{r}
# bind all rows of dataframes in list
df_unemp <- bind_rows(l_out_formatted)

# check it
glimpse(df_unemp)
```

Done! The result is an organized `dataframe` in the long format, ready be to analyzed. 


## Removing Outliers

A recurrent issue in data analysis is handling extreme data points, the so-called _outliers_. 

One way to remedy outliers is to identify potential extreme values with function `quantile`, which returns the value that sets the limit of a cumulative probability. For example, if we want to know which value from `sim_y_with_outlier` that sets the cumulative distribution limit at 95%, we use the following code:

```{r}
# set seed for reproducibility
set.seed(100)

# set options
nT <- 100
sim_x <- rnorm(nT)
my_beta <- 0.5

# simulate x and y
sim_y <- sim_x*my_beta + rnorm(nT)
sim_y_with_outlier <- sim_y

# simulate y with outlier
sim_y_with_outlier[10] <- 50

# find the value of vector that sets the 95% quantile
quantile95 <- quantile(x = abs(sim_y_with_outlier),
                       probs = 0.95)

print(quantile95)
```

Here, the value of `r quantile95` is higher than 95% of all other values available in `sim_y_with_outlier`. Now, we can use this information to find the extreme cases at the tail of the distribution:

```{r}
# find cases higher than 95% quantile
idx <- which(sim_y_with_outlier > quantile95)
print(sim_y_with_outlier[idx])
```

We find the "artificial" outlier we've set in previous code, plus two more cases. We can set how rigorous we are in finding outliers by changing the argument `probs` in `quantile`.

Finally, we need to treat outliers. We can either set it as `NA` or remove it from the vector:

```{r}
# copy content
sim_y_without_outlier <- sim_y_with_outlier

# set NA in outlier
sim_y_without_outlier[idx] <- NA

# or remove it
sim_y_without_outlier <- sim_y_without_outlier[-idx]
```

An alternative for identifying extreme values is the use of statistical tests. For that, the package `outlier` provides functions to identify and remove these cases based on the average distance of each element. See an example next:

```{r, eval=FALSE}
library(outliers)

# find outlier
my_outlier <- outlier(sim_y_with_outlier)

# print it
print(my_outlier)
```

```{r, include=FALSE, purl=FALSE}
sim_y_with_outlier[10]
```


As expected, it correctly identified the outlier. 


### Treating Outliers in `dataframes`

Let's go a bit deeper. In a real data analysis situation, often we need to treat outliers in several columns of a `dataframe`, and not a single vector.

The first step is to define a function that accepts a numeric column and a probability as input, returning the original vector with the extreme cases replaced by `NA`. 

```{r}
replace_outliers <- function(col_in, my_prob = 0.05) {
  # Replaces outliers from a vector and returns a new
  # vector
  #
  # INPUTS: col_in The vector
  #         my_prob Probability of quantiles 
  #                 (will remove quantiles at p and 1-p)
  #
  # OUTPUT: A vector without the outliers
  
  # return if class is other than numeric
  if (!(class(col_in) %in% 
        c('numeric', 'integer'))) return(col_in)
  
  my_outliers <- stats::quantile(x = col_in,
                                 probs = c(my_prob, 1-my_prob),
                                 na.rm = TRUE)
  
  idx <- (col_in <= my_outliers[1])|(col_in >= my_outliers[2])
  col_in[idx] <- NA
  
  return(col_in)
  
}
```

Let's test it:

```{r}
# set test vector
my_x <- runif(25)

# find and replace outliers
print(replace_outliers(my_x, my_prob = 0.05))
```

As we can see, it performed correctly. The output vector has some `NA` elements, which were _outliers_ in the original vector. Now, let's use this function in a dataframe. First, we will again use package `wakefield` to create a `dataframe` with columns of different classes:

```{r}
library(wakefield)
library(tidyverse)

# options
n_obs <- 100

# create extensive random data
my_df <- r_data_frame(n = n_obs,
                      race,
                      age,
                      birth, 
                      height_cm,
                      sex) 

# check it
glimpse(my_df)
```

Now, let's use `purrr::map` to iterate all elements (columns) of `my_df`, recreating all vectors without  outliers:

```{r}
library(purrr)

# remove outlivers from vectors
l_out <- map(my_df, replace_outliers)
```

Next, we regroup all vectors into a single dataframe with `dplyr::as_tibble`: \index{dplyr!as\_tibble}

```{r}
# rebuild dataframe
my_df_no_outlier <- as_tibble(l_out)

# check it
glimpse(my_df_no_outlier)

# summary of my_df_no_outlier
summary(my_df_no_outlier)
```

Note that, as expected, we find `NA` values for the numeric columns `Age` and `Height(cm)`. 

For last, we remove all rows with outliers using `base::na.omit`: \index{base!na.omit}

```{r}
# remove outliers
my_df_no_outlier <- na.omit(my_df_no_outlier)

glimpse(my_df_no_outlier)
```


## Inflation and Price Data

A common effect in economic and financial data is inflation. 

To offset the effect of inflation on price data, the first step is to import a  benchmark for inflation rates. Here we will use the case of the USA, where the most popular inflation index is CPI (consumer price index), reported by the U.S. Bureau of Labor Statistics (BLS) on a monthly basis. Here we will use `Quandl` to download CPI data since 2000:

```{r}
library(GetQuandlData)
library(tidyverse)

# set api (you need your OWN from www.quandl.com)
my_api_key <- readLines(
  '~/Dropbox/98-pass_and_bash/.quandl_api.txt'
  )

# set symbol and dates
my_symbol <- 'RATEINF/INFLATION_USA'
first_date <- as.Date('2000-01-01')
last_date <- Sys.Date()

# get data!
df_inflation <- get_Quandl_series(id_in = my_symbol,
                                  api_key = my_api_key, 
                                  first_date = first_date, 
                                  last_date = last_date)

# sort by date
df_inflation <- df_inflation %>%
  arrange(ref_date)

# check content
glimpse(df_inflation)
```

Now, let's create a random dataframe with random prices:

```{r}
n_T <- nrow(df_inflation)

# create df with prices
my_df <- tibble(Date = df_inflation$ref_date,
                x = 100 + cumsum(rnorm(n_T)),
                y = 100 + cumsum(rnorm(n_T)))

# check it
glimpse(my_df)
```

The first step is to create a deflator index based on the last period. Since the CPI data is already in percentage format, we can just accumulate it assuming an initial value of 1 and dividing the result by the value found on the base date:

```{r}
# accumulate: R_a = cumprod(r_t + 1)
my_df$infl_idx <- cumprod(df_inflation$value/100 +1)

# set inflation index
my_df$infl_idx <- my_df$infl_idx/my_df$infl_idx[nrow(my_df)]
```

And now we create the new variables:

```{r}
my_df$x_desinflated <- my_df$x*my_df$infl_idx
my_df$y_desinflated <- my_df$y*my_df$infl_idx

glimpse(my_df)
```

Done. Following the previous example, we could apply `purrr::map` for all numeric columns of a dataframe.


## Modifying Time Frequency and Aggregating Data

Sometimes we receive data with a mismatch of time frequency, i.e., daily instead of monthly. Here we will show how to change the time frequency of the data, from low to high: daily to weekly, monthly to annual data or the opposite. 

Let's start with an example with the SP500 index, which is available on the daily frequency and will be transformed into an annual table. The first step is to download the daily data with `BatchGetSymbols`.

```{r}
library(BatchGetSymbols)

df_SP500 <- BatchGetSymbols(tickers = '^GSPC',
                            first.date = '2010-01-01',
                            freq.data = 'daily',
                            last.date = '2018-01-01')[[2]]
```

Every time-frequency operation from higher to lower is a simple split-apply-combine type of calculation. For that, we can use the `dplyr` package and `group_by` and `summarise` functions. See the following steps:

```{r}
# from daily to annual
df_SP500_annual <- df_SP500 %>%
  mutate(ref_year = lubridate::year(ref.date)) %>%
  group_by(ref_year) %>%
  summarise(last_value = last(price.adjusted)) 

# glimpse it
glimpse(df_SP500_annual)
```

We will create a new column with the years, group the data according to it and, finally, we searched for the latest available SP500 value. 


## Exercises

01. Consider the following `dataframe`:

```{r}
library(tidyverse)

my_N <- 100

df <- bind_rows(tibble(ticker = rep('STOCK 1', my_N),
                       ref_date = Sys.Date() + 1:my_N,
                       price = 100 + cumsum(rnorm(my_N))),
                tibble(ticker = rep('STOCK 2', my_N),
                       ref_date = Sys.Date() + 1:my_N,
                       price = 100 + cumsum(rnorm(my_N))) )

print(df)
```

The format is long or wide? Explain your answer.

02. Modify the format of the previous dataframe, from long to wide or vice-versa.

03. Consider the following `list`:

```{r}
my_l <- list(df1 = tibble(x = 1:100, 
                          y = runif(100)),
             df2 = tibble(x = 1:100, 
                          y = runif(100), 
                          v = runif(100)),
             df3 = tibble(x = 1:100, 
                          y = runif(100), 
                          z = runif(100)) )
```

Aggregate all elements of `my_l` into a single dataframe. So, the question is, what happened to the data points in `df1` for columns `v` and `z`? 

04. Use package `BatchGetSymbols` to download SP500 index data (`'^GSPC'`) from 1950-01-01 to today. What are the top 5 absolute returns (positive or negative) of the index? Create and present on screen a `dataframe` with the values and dates of these extreme returns.

05. Use the function created in this chapter for removing outliers from the SP500 data with `p = 0.025`. How many rows were lost in this process?

06. Use function `BatchGetSymbols::BatchGetSymbols` to download FTSE index prices (`'^FTSE'`) from `'2010-01-01'` to the present day. Next, build a dataset of index values in the annual frequency by looking at the latest available index value for each year. Tip: see function `dplyr::summarise_all` for a functional way of aggregating all columns.

07. Use the same daily data from the previous exercise and build a new dataset in the monthly frequency.

08. CHALLENGE - For the previously downloaded FTSE daily data, check the dates and prices of the 20 biggest price drops. If an investor bought the index at the prices of the biggest drops and maintained it for 30 days, what would be his average nominal return per transaction?
